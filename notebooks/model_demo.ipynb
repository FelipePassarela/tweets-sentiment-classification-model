{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LSTMSentimentClassifier                  [64, 3]                   --\n",
       "├─Embedding: 1-1                         [64, 128, 768]            38,603,520\n",
       "├─Dropout: 1-2                           [64, 128, 768]            --\n",
       "├─LSTM: 1-3                              [64, 128, 512]            5,255,168\n",
       "├─Dropout: 1-4                           [64, 512]                 --\n",
       "├─Linear: 1-5                            [64, 3]                   1,539\n",
       "==========================================================================================\n",
       "Total params: 43,860,227\n",
       "Trainable params: 43,860,227\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 45.52\n",
       "==========================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 83.89\n",
       "Params size (MB): 175.44\n",
       "Estimated Total Size (MB): 259.46\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from src.utils.getters import get_config\n",
    "from src.utils.model_loader import load_model\n",
    "from src.models.lstm_sentiment_classifier import LSTMSentimentClassifier\n",
    "\n",
    "config = get_config(config_path=\"../config.yaml\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# path = \"../\" + config[\"training\"][\"early_stopping\"][\"checkpoint_path\"]\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(config[\"data\"][\"tokenizer\"][\"name\"])\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"data\"][\"tokenizer\"][\"name\"])\n",
    "model = LSTMSentimentClassifier(vocab_size=tokenizer.vocab_size,\n",
    "                                embedding_dim= config[\"model\"][\"embedding_dim\"],\n",
    "                                hidden_dim=    config[\"model\"][\"hidden_dim\"],\n",
    "                                output_dim=    config[\"model\"][\"output_dim\"],\n",
    "                                n_layers=      config[\"model\"][\"n_layers\"],\n",
    "                                bidirectional= config[\"model\"][\"bidirectional\"],\n",
    "                                dropout=       config[\"model\"][\"dropout\"])\n",
    "path = Path(\"../\") / config[\"training\"][\"early_stopping\"][\"checkpoint_path\"] / \"LSTMSentimentClassifier.pt\"\n",
    "model = load_model(model, path, device)\n",
    "model.eval()\n",
    "\n",
    "sample_input = {\n",
    "    \"input_ids\": torch.zeros((config[\"training\"][\"batch_size\"], \n",
    "                              config[\"data\"][\"tokenizer\"][\"max_length\"]), \n",
    "                              dtype=torch.long, device=device),\n",
    "    \"attention_mask\": torch.ones((config[\"training\"][\"batch_size\"], \n",
    "                                  config[\"data\"][\"tokenizer\"][\"max_length\"]), \n",
    "                                  dtype=torch.long, device=device)}\n",
    "summary(model, input_data=sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   495,  6804,   176,     2,   133,  2038,   385,  6804,   132,\n",
      "          1310,    16, 23523, 28976,     8,    38,  2198,  2814,    24,     4,\n",
      "          1437, 50118, 50118, 14181,     5, 11671,    66,     4,     2,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 15691, 11428,  3290,  2459, 38856, 21109,   261,     2, 19224,\n",
      "         18636, 10659, 18636, 10659, 18636, 10659,    42,     5,  7319,    38,\n",
      "           437,  1686,    59,  5278,   298,     4, 11087,  2133,   310,   137,\n",
      "            38,   471,    88,   173,  1717,  4147,     4,   978,    38, 16112,\n",
      "          2067,  8103, 10470, 11582,  6569, 10470, 11582,   849, 41796,   241,\n",
      "          3865, 10339,  2300,  9084, 17654,     4,   175,    73,   642,    73,\n",
      "         25388, 21738,   605, 37426,   597, 10232,   705,    73,  1174,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   487, 47435,     2,  8987,   769,    12, 39161, 15961, 40238,\n",
      "          2377,   634,    42,  4704,   114,    12,  3654,    12, 29225,    12,\n",
      "         13040,    12, 22303,     4,   175,    73, 14420,    73, 21967,  4330,\n",
      "            12, 37594,  1174,     8,   734,   117,     6,   787,  7051, 31587,\n",
      "            16,   202, 16396,  4740,    19,   787, 22699,  1916,     2,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 20653, 10643,   495, 27237,     2, 20653,     9,  4053, 14578,\n",
      "            35, 24579, 17884,     5,   256,   306,   262,   498, 50118, 50118,\n",
      "         28407,    29,  7324,     6,   817, 18735,   757,  3983,   479, 30628,\n",
      "         12103, 14153,  8103,  8210, 20024,     2,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   448, 23004, 12048,     2, 41541,   177,   787,   717,  2620,\n",
      "         23004, 12048, 26964,  8384,  1893,     4,  1556,     4,   175,    73,\n",
      "         11365,   846,   119, 32774, 26979,     2,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]],\n",
      "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}\n",
      "tensor([[ 0.0045,  0.3075, -0.2485],\n",
      "        [-0.2387,  0.3097,  0.0792],\n",
      "        [-0.6345,  0.5653,  0.3492],\n",
      "        [ 0.2046,  0.1274, -0.3305],\n",
      "        [ 1.3739, -0.9137, -0.7811]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sep = tokenizer.sep_token\n",
    "msgs = [f\"Dota2{sep}The professional dota 2 scene is fucking exploding and I completely welcome it. \\n\\nGet the garbage out.\",\n",
    "        f\"TomClancysGhostRecon{sep}See😒😒😒 this the mess I'm talking about smh. Tryna play before I head into work ugh. Now I gotta wait 🤬🤬 #ghostreconbreakpoint instagram.com/p/CFKOwBiFdrv/…\",\n",
    "        f\"Nvidia{sep}just re-installed NVidia drivers using this guide if-not-true-then-false.com/2015/fedora-nv… and... no, @firefox is still laggy with @figma\",\n",
    "        f\"CallOfDuty{sep}Call of duty logic: Nerfs the M4 7 times\\n\\nKeeps helicopter, makes akimbo .357 stupid cheating 🥴\",\n",
    "        f\"MaddenNFL{sep}Nice game @EAMaddenNFL 👍 pic.twitter.com/csVm607lov\"]\n",
    "\n",
    "inputs = tokenizer(msgs, \n",
    "                   max_length=config[\"data\"][\"tokenizer\"][\"max_length\"],\n",
    "                   padding=\"max_length\", \n",
    "                   truncation=True, \n",
    "                   return_tensors=\"pt\",\n",
    "                   return_token_type_ids=True).to(device)\n",
    "outputs = model(**inputs)\n",
    "outputs = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
    "\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Dota2</s>The professional dota 2 scene is fucking exploding and I completely welcome it. \n",
      "\n",
      "Get the garbage out.\" is Neutral (43.25%)\n",
      "\"TomClancysGhostRecon</s>See😒😒😒 this the mess I'm talking about smh. Tryna play before I head into work ugh. Now I gotta wait 🤬🤬 #ghostreconbreakpoint instagram.com/p/CFKOwBiFdrv/…\" is Neutral (42.16%)\n",
      "\"Nvidia</s>just re-installed NVidia drivers using this guide if-not-true-then-false.com/2015/fedora-nv… and... no, @firefox is still laggy with @figma\" is Neutral (47.46%)\n",
      "\"CallOfDuty</s>Call of duty logic: Nerfs the M4 7 times\n",
      "\n",
      "Keeps helicopter, makes akimbo .357 stupid cheating 🥴\" is Negative (39.82%)\n",
      "\"MaddenNFL</s>Nice game @EAMaddenNFL 👍 pic.twitter.com/csVm607lov\" is Negative (82.14%)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for msg, probas in zip(msgs, F.softmax(outputs, dim=1)):\n",
    "    class_decoder = {v: k for k, v in config[\"data\"][\"label_map\"].items()}\n",
    "    proba = probas.max().item()\n",
    "    pred = class_decoder[probas.argmax().item()]\n",
    "    print(f'\"{msg}\" is {pred} ({proba:.2%})')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
